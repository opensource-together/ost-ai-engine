# Plateforme d'Analyse de repos Git

Un syst√®me distribu√© pour l'analyse automatis√©e de repos Git qui collecte quotidiennement les donn√©es de repos, les traite via des pipelines d'apprentissage automatique, et stocke les insights s√©mantiques dans une base de donn√©es vectorielle pour des requ√™tes intelligentes.

## üéØ Aper√ßu du Projet

Cette plateforme automatise le processus de :
1. **Collecte Quotidienne de D√©p√¥ts** - R√©cup√®re les donn√©es de commits, modifications de fichiers et m√©tadonn√©es depuis les repos Git configur√©s
2. **Analyse par IA** - Traite les donn√©es Git brutes √† travers des mod√®les d'apprentissage automatique pour extraire des insights
3. **Stockage Vectoriel** - Stocke les donn√©es analys√©es dans une base de donn√©es vectorielle pour la recherche s√©mantique et les requ√™tes de similarit√©
4. **Traitement Scalable** - Utilise des files d'attente de t√¢ches distribu√©es pour g√©rer efficacement un grand nombre de repos

## üìÅ Structure du Projet

```
src/
‚îú‚îÄ‚îÄ domain/                     # Logique m√©tier centrale (aucune d√©pendance externe)
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Entit√©s du domaine
‚îÇ   ‚îî‚îÄ‚îÄ ports/                  # D√©finitions d'interfaces
‚îú‚îÄ‚îÄ application/                # Cas d'usage et workflows m√©tier
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Services m√©tier centraux
‚îÇ   ‚îî‚îÄ‚îÄ use_cases/              # Op√©rations m√©tier sp√©cifiques
‚îú‚îÄ‚îÄ infrastructure/             # Adaptateurs syst√®mes externes
‚îÇ   ‚îú‚îÄ‚îÄ postgres/               # Persistance base de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ redis/                  # File d'attente et cache
‚îÇ   ‚îú‚îÄ‚îÄ celery/                 # Traitement t√¢ches distribu√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Configuration application Celery
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration Celery
‚îÇ   ‚îú‚îÄ‚îÄ scraping/               # Impl√©mentations extraction donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ analysis/               # Impl√©mentations traitement IA
‚îú‚îÄ‚îÄ api/                        # Interfaces externes
‚îú‚îÄ‚îÄ config/                     # Gestion configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Param√®tres application
‚îî‚îÄ‚îÄ tests/                      # Suites de tests
    ‚îú‚îÄ‚îÄ unit/                   # Tests unitaires par couche
    ‚îú‚îÄ‚îÄ integration/            # Tests d'int√©gration
    ‚îî‚îÄ‚îÄ fixtures/               # Donn√©es de test
```

## üöÄ D√©marrage Rapide

### Pr√©requis

- Python 3.9+
- Conda 24.11
- PostgreSQL 17+
- Redis 6+
- Git

### Installation

1. **Cloner le d√©p√¥t**
   ```bash
   git clone https://github.com/opensource-together/data-engine
   cd data-engine
   ```

2. **Configurer l'environnement Python**
   ```bash
   conda create -n data-engine python=3.9
   conda activate data-engine
   pip install -r requirements.txt
   ```

3. **Configurer l'environnement**
   ```bash
   cp .env.example .env
   # √âditer .env avec vos identifiants de base de donn√©es et param√®tres
   ```

### Configuration

Cr√©er un fichier `.env` avec les param√®tres requis :

```env
# Base de donn√©es
DATABASE_URL=postgresql://utilisateur:motdepasse@localhost:5432/analyse_git
REDIS_URL=redis://localhost:6379/0

# Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_URL=redis://localhost:6379/0

# Analyse IA
VECTOR_DB_TYPE=chromadb  # ou pinecone
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Surveillance
LOG_LEVEL=INFO
FLOWER_PORT=5555
```

## üß™ Workflow de D√©veloppement

### Ex√©cution des Tests

#### Tests Unitaires de l'Infrastructure (76 tests)
```bash
# Tous les tests d'infrastructure
poetry run pytest tests/unit/infrastructure/ -v

# Tests sp√©cifiques par composant
poetry run pytest tests/unit/infrastructure/postgres/ -v      # Database (22 tests)
poetry run pytest tests/unit/infrastructure/scraping/ -v      # GitHub Scraper (12 tests)
poetry run pytest tests/unit/infrastructure/test_config.py -v # Configuration (18 tests)
poetry run pytest tests/unit/infrastructure/analysis/ -v      # Model Persistence (22 tests)
```

#### Tests d'Int√©gration
```bash
# Tests d'int√©gration complets
poetry run pytest tests/integration/ -v

# Tests avec couverture
poetry run pytest --cov=src --cov-report=html
```

#### Validation Pr√©-commit
```bash
# Qualit√© du code
poetry run ruff check .
poetry run black --check .

# Tests critiques
poetry run pytest tests/unit/infrastructure/ -v
```

**üìä Statut des Tests**: ‚úÖ **76/76 tests d'infrastructure passent**
**üìñ Documentation**: Voir [docs/testing_strategy.md](docs/testing_strategy.md) pour les d√©tails

### √âtendre les Collecteurs

Pour ajouter le support de nouveaux types de sources :

1. **Cr√©er un nouveau collecteur** dans `infrastructure/scraping/` :
   ```python
   class NouveauCollecteur(Scraper):
       def can_handle(self, source: DataSource) -> bool:
           return source.source_type == SourceType.NOUVEAU_TYPE
       
       def scrape(self, source: DataSource) -> ScrapedData:
           # Impl√©mentation
   ```

2. **L'enregistrer** dans `scraper_factory.py` :
   ```python
   self._scrapers.append(NouveauCollecteur())
   ```

### Ajouter des √âtapes d'Analyse IA

1. **Impl√©menter service d'analyse** dans `infrastructure/analysis/` :
   ```python
   class ServiceAnalysePersonnalis√©(AnalysisService):
       def analyze(self, data: ScrapedData) -> AnalysisResult:
           # Votre traitement IA
   ```

2. **Mettre √† jour le workflow** pour inclure votre √©tape d'analyse

## üîß Options de Configuration

### Configuration de Collecte de D√©p√¥t

```json
{
  "branch": "main",
  "since_date": "2024-01-01T00:00:00Z",
  "file_extensions": [".py", ".js"],
  "max_commits": 1000,
  "include_merge_commits": false,
  "exclude_paths": ["tests/", "docs/"]
}
```

### Configuration Celery

```python
# Ajuster concurrence workers selon votre syst√®me
CELERY_WORKER_CONCURRENCY = 4

# Routage des t√¢ches pour diff√©rents types de travail
CELERY_ROUTES = {
    'scraping.*': {'queue': 'scraping'},
    'analysis.*': {'queue': 'analysis'},
}
```
